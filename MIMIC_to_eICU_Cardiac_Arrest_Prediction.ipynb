{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MIMIC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "file_path = r\"E:\\data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of samples before preprocessing\n",
    "print(\"Number of samples before preprocessing:\", len(data))\n",
    "\n",
    "# Data preprocessing\n",
    "# Filter: Age >= 18\n",
    "filtered_data1 = data[data['age'] >= 18]\n",
    "print(\"Number of samples after filtering (Age >= 18):\", len(filtered_data1))\n",
    "\n",
    "# Filter: Exclude myocardial infarction patients\n",
    "filtered_data2 = filtered_data1[filtered_data1['myocardial_infarct'] != 1]\n",
    "print(\"Number of samples after filtering (Non-myocardial infarction):\", len(filtered_data2))\n",
    "\n",
    "# Filter: Exclude congestive heart failure patients\n",
    "filtered_data3 = filtered_data2[filtered_data2['congestive_heart_failure'] != 1]\n",
    "print(\"Number of samples after filtering (Non-congestive heart failure):\", len(filtered_data3))\n",
    "\n",
    "# Filter: ICU length of stay >= 0.125 days (3 hours)\n",
    "filtered_data4 = filtered_data3[filtered_data3['los_icu'] >= 0.125]\n",
    "print(\"Number of samples after filtering (ICU LOS >= 3 hours):\", len(filtered_data4))\n",
    "\n",
    "# Final dataset\n",
    "filtered_data = filtered_data4\n",
    "\n",
    "# Print final label distribution\n",
    "label_distribution = filtered_data['label'].value_counts()\n",
    "print(\"\\nLabel distribution in the final dataset:\")\n",
    "print(label_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def process_race(df, race_column='race'):\n",
    "    white_keywords = ['WHITE', 'CAUCASIAN', 'PORTUGUESE', 'EUROPEAN', 'RUSSIAN', 'BRAZILIAN']\n",
    "    black_keywords = ['BLACK', 'AFRICAN']\n",
    "    asian_keywords = ['ASIAN', 'CHINESE', 'KOREAN', 'INDIAN']\n",
    "    hispanic_keywords = ['HISPANIC', 'LATINO', 'SALVADORAN', 'GUATEMALAN', 'HONDURAN', \n",
    "                        'MEXICAN', 'DOMINICAN', 'COLUMBIAN', 'CUBAN', 'PUERTO RICAN',\n",
    "                        'SOUTH AMERICAN', 'CENTRAL AMERICAN']\n",
    "    \n",
    "    def categorize_race(x):\n",
    "        if pd.isna(x):\n",
    "            return 'Other'\n",
    "        x = str(x).upper()\n",
    "        if any(keyword in x for keyword in white_keywords):\n",
    "            return 'White'\n",
    "        elif any(keyword in x for keyword in black_keywords):\n",
    "            return 'Black'\n",
    "        elif any(keyword in x for keyword in asian_keywords):\n",
    "            return 'Asian'\n",
    "        elif any(keyword in x for keyword in hispanic_keywords):\n",
    "            return 'Hispanic'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    return df[race_column].apply(categorize_race)\n",
    "\n",
    "# Separate features and labels\n",
    "features = filtered_data[['gender', 'age', 'race',\n",
    "'d_dimer', 'fibrinogen', 'thrombin', 'inr', 'pt', 'ptt', \n",
    "'albumin', 'globulin', 'chloride', 'aniongap', 'bicarbonate', \n",
    "'calcium', 'total_protein', 'bun', 'potassium', 'sodium', 'glucose', 'creatinine',\n",
    "'alt','alp','ast','bilirubin_indirect','ggt','ck_mb','ck_cpk','bilirubin_direct','amylase','ld_ldh','bilirubin_total',\n",
    "'monocytes', 'nrbc','metamyelocytes','immature_granulocytes','bands','atypical_lymphocytes','neutrophils','lymphocytes',\n",
    "'basophils','neutrophils_abs','monocytes_abs','lymphocytes_abs','eosinophils_abs','basophils_abs','wbc','eosinophils',\n",
    "'mchc','rdw','rbc','platelet','mcv','rdwsd','mch','hemoglobin','hct', 'so2', 'glucose','lactate','potassium','temperature',\n",
    "'calcium','chloride','carboxyhemoglobin','hemoglobin','hct','methemoglobin','bicarbonate','po2','totalco2',\n",
    "'fio2_chartevents','fio2','aado2','pco2','pao2fio2ratio','ph','baseexcess','aado2_calc','a','a_unit','b','b_unit','c','c_unit','d','d_unit','e','e_unit','f','f_unit','g','g_unit','h','h_unit',\n",
    "'weight','height','hr','sbp','sbp_ni','dbp','dbp_ni','mbp','mbp_ni','rr','temperature','spo2','glucose',\n",
    "'age_score','peripheral_vascular_disease','cerebrovascular_disease','dementia','chronic_pulmonary_disease','rheumatic_disease',\n",
    "'peptic_ulcer_disease','mild_liver_disease','diabetes','paraplegia','renal_disease','severe_liver_disease',\n",
    "'metastatic_solid_tumor','aids','gcs']]\n",
    "labels = filtered_data['label']\n",
    "\n",
    "\n",
    "# Processing racial data\n",
    "features['race'] = process_race(features)\n",
    "print(features['race'].value_counts())\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Delete samples with null gender\n",
    "features = features.dropna(subset=['gender'])\n",
    "labels = labels[features.index]\n",
    "\n",
    "# Save original index\n",
    "original_index = features.index\n",
    "\n",
    "# Print initial data information\n",
    "print(\"Initial data information：\")\n",
    "print(f\"Number of features: {features.shape[1]}\")\n",
    "print(f\"Number of samples: {features.shape[0]}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count the number and proportion of missing values in each column\n",
    "missing_counts_before = features.isna().sum()\n",
    "missing_ratio_before = (missing_counts_before / len(features) * 100)\n",
    "missing_ratio_sorted = missing_ratio_before[missing_ratio_before > 0].sort_values(ascending=False)\n",
    "\n",
    "# Draw distribution map of missing values\n",
    "plt.figure(figsize=(10, 12))\n",
    "bars = plt.barh(missing_ratio_sorted.index, missing_ratio_sorted, \n",
    "                color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add proportion label\n",
    "for bar, ratio in zip(bars, missing_ratio_sorted):\n",
    "    plt.text(ratio + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "             f\"{ratio:.1f}%\", va='center', fontsize=9)\n",
    "\n",
    "plt.title(\"Missing value distribution (before processing)\", fontsize=14)\n",
    "plt.xlabel(\"Percentage of missing values (%)\", fontsize=12)\n",
    "plt.ylabel(\"Characteristic\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Delete high missing rate columns (threshold 30%)\n",
    "threshold = 0.3 * len(features)\n",
    "columns_to_drop = features.columns[features.isna().sum() > threshold]\n",
    "features = features.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Delete columns with a missing rate exceeding 30%, removing {len (columns_to_drop)} features in total:\")\n",
    "print(columns_to_drop.tolist())\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Standardize numerical features\n",
    "numeric_features = features.select_dtypes(include=[np.number]).columns\n",
    "scalers = {}\n",
    "for feature in numeric_features:\n",
    "    scalers[feature] = StandardScaler()\n",
    "    features.loc[:, feature] = scalers[feature].fit_transform(features[[feature]].fillna(0))\n",
    "\n",
    "# Encode categorical variables\n",
    "# gender coding\n",
    "if 'gender' in features.columns:\n",
    "    gender_mapping = {\n",
    "        'F': 0, 'f': 0, \n",
    "        'M': 1, 'm': 1,\n",
    "        '0': 0, '0.0': 0, 0: 0, 0.0: 0,\n",
    "        '1': 1, '1.0': 1, 1: 1, 1.0: 1\n",
    "    }\n",
    "\n",
    "    print(features['gender'].unique())\n",
    "    features['gender'] = features['gender'].map(gender_mapping)\n",
    "    \n",
    "    print(features['gender'].value_counts())\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# racial coding\n",
    "if 'race' in features.columns:\n",
    "    le = LabelEncoder()\n",
    "    features['race'] = le.fit_transform(features['race'])\n",
    "    for i, label in enumerate(le.classes_):\n",
    "        print(f\"{label}: {i}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Using KNNImputer to fill in missing values\n",
    "imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "features_imputed_array = imputer.fit_transform(features)\n",
    "\n",
    "features_imputed = pd.DataFrame(features_imputed_array, \n",
    "                              columns=features.columns,\n",
    "                              index=original_index)\n",
    "\n",
    "for feature in numeric_features:\n",
    "    if feature in features_imputed.columns:\n",
    "        features_imputed.loc[:, feature] = scalers[feature].inverse_transform(features_imputed[[feature]])\n",
    "\n",
    "remaining_missing = features_imputed.isnull().sum()\n",
    "if remaining_missing.sum() > 0:\n",
    "    print(\"Warning: Columns with missing values still exist after filling：\")\n",
    "    print(remaining_missing[remaining_missing > 0])\n",
    "else:\n",
    "    print(\"All missing values have been successfully filled in\")\n",
    "\n",
    "features = features_imputed\n",
    "\n",
    "assert len(features) == len(labels), \"Feature and label length do not match\"\n",
    "assert (features.index == labels.index).all(), \"The indexes of features and labels do not match\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Final data processing result：\")\n",
    "print(f\"Retain the number of features: {features.shape[1]}\")\n",
    "print(f\"Retain sample quantity: {features.shape[0]}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_features = features.columns.tolist()\n",
    "print(\"\\nReserved feature list：\")\n",
    "print(final_features)\n",
    "\n",
    "# Basic statistical information\n",
    "print(\"\\nBasic statistical information of features：\")\n",
    "print(features.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class distribution before downsampling\n",
    "print(\"\\nClass distribution before downsampling:\")\n",
    "print(labels.value_counts())\n",
    "\n",
    "# Get data for each class\n",
    "df_0 = features[labels == 0]\n",
    "df_1 = features[labels == 1]\n",
    "\n",
    "# Get the number of samples in the minority class\n",
    "num_samples = len(df_1)\n",
    "print(f\"\\nNumber of minority class samples: {num_samples}\")\n",
    "\n",
    "# Downsample the majority class\n",
    "df_0_downsampled = df_0.sample(n=num_samples, random_state=42)\n",
    "\n",
    "# Combine the downsampled majority class with the minority class\n",
    "features_balanced = pd.concat([df_0_downsampled, df_1])\n",
    "labels_balanced = pd.concat([pd.Series(0, index=df_0_downsampled.index),\n",
    "                             pd.Series(1, index=df_1.index)])\n",
    "\n",
    "# Shuffle the combined data\n",
    "idx = np.random.permutation(len(features_balanced))\n",
    "features = features_balanced.iloc[idx].reset_index(drop=True)\n",
    "labels = labels_balanced.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "# Print class distribution after downsampling\n",
    "print(\"\\nClass distribution after downsampling:\")\n",
    "print(labels.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the specified features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize continuous features\n",
    "features_to_normalize = ['age', 'inr', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc',\n",
    "                         'mchc', 'rdw', 'rbc', 'platelet', 'mcv', 'mch', 'weight']\n",
    "\n",
    "# Extract features to be normalized\n",
    "features_to_normalize_data = features[features_to_normalize]\n",
    "\n",
    "# Save original features for reference\n",
    "original_features = features.copy()\n",
    "\n",
    "# Perform normalization\n",
    "scaler = StandardScaler()\n",
    "features_to_normalize_scaled = scaler.fit_transform(features_to_normalize_data)\n",
    "\n",
    "# Convert normalized data to DataFrame\n",
    "features_to_normalize_scaled_df = pd.DataFrame(features_to_normalize_scaled,\n",
    "                                                columns=features_to_normalize,\n",
    "                                                index=features.index)\n",
    "\n",
    "# Update the features DataFrame with normalized values\n",
    "features.update(features_to_normalize_scaled_df)\n",
    "\n",
    "# Check missing values after normalization\n",
    "missing_values_after_normalization = features.isna().sum()\n",
    "print(\"\\nMissing value statistics after normalization:\")\n",
    "print(missing_values_after_normalization[missing_values_after_normalization > 0])\n",
    "\n",
    "print(\"\\nFeatures data after normalization:\")\n",
    "print(features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boruta feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from boruta import BorutaPy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_sens = recall_score(y_train, y_train_pred)\n",
    "    test_sens = recall_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_spec = recall_score(y_train, y_train_pred, pos_label=0)\n",
    "    test_spec = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "    \n",
    "    train_auroc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "    test_auroc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    train_ppv = precision_score(y_train, y_train_pred)\n",
    "    test_ppv = precision_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_npv = precision_score(y_train, y_train_pred, pos_label=0)\n",
    "    test_npv = precision_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    return train_acc, test_acc, train_sens, test_sens, train_spec, test_spec, train_auroc, test_auroc, train_ppv, test_ppv, train_npv, test_npv, train_f1, test_f1, y_test, y_test_pred, model\n",
    "\n",
    "\n",
    "def boruta_feature_selection(X, y, feature_names):\n",
    "\n",
    "    # Create a copy of the data\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # Process categorical variables\n",
    "    for column in X_processed.columns:\n",
    "        if X_processed[column].dtype == 'object' or X_processed[column].dtype == 'category':\n",
    "            # Label encode categorical variables\n",
    "            X_processed[column] = pd.Categorical(X_processed[column]).codes\n",
    "        elif pd.api.types.is_bool_dtype(X_processed[column]):\n",
    "            # Convert boolean values to integers\n",
    "            X_processed[column] = X_processed[column].astype(int)\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    X_processed = X_processed.astype(float)\n",
    "    \n",
    "    # Use Random Forest as the base model for Boruta\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "    \n",
    "    # Initialize Boruta\n",
    "    boruta = BorutaPy(rf, n_estimators='auto', random_state=42)\n",
    "    \n",
    "    # Train Boruta\n",
    "    boruta.fit(X_processed.values, y)\n",
    "    \n",
    "    # Get selected and rejected features\n",
    "    selected_features = boruta.support_\n",
    "    rejected_features = boruta.support_weak_\n",
    "    \n",
    "    # Print feature selection results\n",
    "    print(\"\\nBoruta Feature Selection Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    for feature, selected in zip(feature_names, selected_features):\n",
    "        print(f\"{feature:<30} {'Selected' if selected else 'Rejected'}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get names of selected and rejected features\n",
    "    selected_feature_names = np.array(feature_names)[selected_features]\n",
    "    rejected_feature_names = np.array(feature_names)[rejected_features]\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_feature_names)} features:\")\n",
    "    for feat in selected_feature_names:\n",
    "        print(f\"- {feat}\")\n",
    "    \n",
    "    print(f\"\\nRejected {len(rejected_feature_names)} features:\")\n",
    "    for feat in rejected_feature_names:\n",
    "        print(f\"- {feat}\")\n",
    "    \n",
    "    # Get feature importance rankings\n",
    "    feature_importance = boruta.ranking_\n",
    "    \n",
    "    # Create DataFrame for importance ranking\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance Rank': feature_importance\n",
    "    }).sort_values('Importance Rank')\n",
    "    \n",
    "    # Plot feature importance ranking\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance Rank', y='Feature', palette=\"Set2\")\n",
    "    plt.title(\"Feature Importance Rankings\", fontsize=16)\n",
    "    plt.xlabel(\"Importance Rank (lower is better)\", fontsize=12)\n",
    "    plt.ylabel(\"Feature\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_features, selected_feature_names\n",
    "\n",
    "\n",
    "def balance_samples(features, labels):\n",
    "    # Get indices for positive and negative samples\n",
    "    neg_indices = labels[labels == 0].index\n",
    "    pos_indices = labels[labels == 1].index\n",
    "    \n",
    "    # Print original class distribution\n",
    "    print(\"\\nOriginal Class Distribution:\")\n",
    "    print(labels.value_counts())\n",
    "    \n",
    "    # Determine the number of samples in the minority class\n",
    "    min_samples = min(len(neg_indices), len(pos_indices))\n",
    "    \n",
    "    # Undersample the majority class\n",
    "    if len(neg_indices) > len(pos_indices):\n",
    "        neg_indices = np.random.choice(neg_indices, min_samples, replace=False)\n",
    "    else:\n",
    "        pos_indices = np.random.choice(pos_indices, min_samples, replace=False)\n",
    "    \n",
    "    # Combine the balanced indices\n",
    "    balanced_indices = np.concatenate([neg_indices, pos_indices])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "    \n",
    "    # Extract balanced dataset\n",
    "    features_balanced = features.loc[balanced_indices]\n",
    "    labels_balanced = labels.loc[balanced_indices]\n",
    "    \n",
    "    # Print class distribution after balancing\n",
    "    print(\"\\nClass Distribution After Balancing:\")\n",
    "    print(labels_balanced.value_counts())\n",
    "    \n",
    "    return features_balanced, labels_balanced\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Sampling hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 200, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 12)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_float('max_features', 0.7, 1.0)\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "    \n",
    "    # Create the model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Repeated Stratified k-fold cross-validation\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "    fold_results = []\n",
    "    gender_labels = features['gender']\n",
    "    \n",
    "    # First, balance the entire dataset\n",
    "    features_balanced, labels_balanced = balance_samples(features, labels)\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(rskf.split(features_balanced, labels_balanced, gender_labels)):\n",
    "        X_train, X_test = features_balanced.iloc[train_index], features_balanced.iloc[test_index]\n",
    "        y_train, y_test = labels_balanced.iloc[train_index], labels_balanced.iloc[test_index]\n",
    "\n",
    "        # Output positive/negative sample counts for training and testing sets\n",
    "        print(f\"\\nFold {fold_idx + 1}:\")\n",
    "        print(\"Training set - Positive samples:\", sum(y_train == 1), \", Negative samples:\", sum(y_train == 0))\n",
    "        print(\"Testing set - Positive samples:\", sum(y_test == 1), \", Negative samples:\", sum(y_test == 0))\n",
    "\n",
    "        # Perform feature selection using Boruta\n",
    "        selected_features, selected_feature_names = boruta_feature_selection(X_train, y_train, features_balanced.columns)\n",
    "        X_train_pls = X_train.loc[:, selected_features]\n",
    "        X_test_pls = X_test.loc[:, selected_features]\n",
    "\n",
    "        # Evaluate the model\n",
    "        results = evaluate_model(model, X_train_pls, y_train, X_test_pls, y_test)\n",
    "        train_acc, test_acc, train_sens, test_sens, train_spec, test_spec, train_auroc, test_auroc, train_ppv, test_ppv, train_npv, test_npv, train_f1, test_f1, y_test, y_test_pred, model = results\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold_idx': fold_idx + 1,\n",
    "            'train_index': train_index,\n",
    "            'test_index': test_index,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'train_sens': train_sens,\n",
    "            'test_sens': test_sens,\n",
    "            'train_spec': train_spec,\n",
    "            'test_spec': test_spec,\n",
    "            'train_auroc': train_auroc,\n",
    "            'test_auroc': test_auroc,\n",
    "            'train_ppv': train_ppv,\n",
    "            'test_ppv': test_ppv,\n",
    "            'train_npv': train_npv,\n",
    "            'test_npv': test_npv,\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'selected_feature_names': selected_feature_names,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'model': model\n",
    "        })\n",
    "    \n",
    "    # Return average test AUROC as the optimization target for Optuna\n",
    "    trial.set_user_attr(\"fold_results\", fold_results)\n",
    "    return np.mean([result['test_auroc'] for result in fold_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLS feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from boruta import BorutaPy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_sens = recall_score(y_train, y_train_pred)\n",
    "    test_sens = recall_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_spec = recall_score(y_train, y_train_pred, pos_label=0)\n",
    "    test_spec = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "    \n",
    "    train_auroc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "    test_auroc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    train_ppv = precision_score(y_train, y_train_pred)\n",
    "    test_ppv = precision_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_npv = precision_score(y_train, y_train_pred, pos_label=0)\n",
    "    test_npv = precision_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    return train_acc, test_acc, train_sens, test_sens, train_spec, test_spec, train_auroc, test_auroc, train_ppv, test_ppv, train_npv, test_npv, train_f1, test_f1, y_test, y_test_pred, model\n",
    "\n",
    "\n",
    "def pls_feature_selection(X, y, feature_names):\n",
    "    pls = PLSRegression(n_components=min(X.shape[1], X.shape[0]-1))\n",
    "    pls.fit(X, y)\n",
    "    \n",
    "\n",
    "    t = pls.x_scores_\n",
    "    q = pls.y_loadings_\n",
    "    VIP = np.sqrt(np.sum((t ** 2) @ (q.T ** 2), axis=0) / np.sum(t ** 2, axis=0))\n",
    "    \n",
    "\n",
    "    print(\"\\nVIP Scores for each feature:\")\n",
    "    print(\"-\" * 40)\n",
    "    for feature, vip in zip(feature_names, VIP):\n",
    "        print(f\"{feature:<30} {vip:.3f}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "\n",
    "    print(f\"\\nVIP Statistics:\")\n",
    "    print(f\"Mean VIP: {np.mean(VIP):.3f}\")\n",
    "    print(f\"Max VIP: {np.max(VIP):.3f}\")\n",
    "    print(f\"Min VIP: {np.min(VIP):.3f}\")\n",
    "\n",
    "    max_vip_index = np.argmax(VIP)  \n",
    "    max_vip_feature = feature_names[max_vip_index]  \n",
    "\n",
    "    print(f\"Max VIP Feature: {max_vip_feature} with VIP: {np.max(VIP):.3f}\")\n",
    "\n",
    "\n",
    "    print(f\"type(VIP): {type(VIP)}\")\n",
    "    print(f\"feature_names: {feature_names}\")\n",
    "\n",
    "    if 'gcs' in feature_names:\n",
    "        gcs_index = feature_names.get_loc('gcs')  \n",
    "        gcs_vip = VIP[gcs_index]  \n",
    "        print(f\"VIP for 'gcs' feature: {gcs_vip:.3f}\")\n",
    "\n",
    "    else:\n",
    "        print(\"'gcs' feature not found in the feature names.\")\n",
    "\n",
    "\n",
    "    selected_features = VIP > 0.5\n",
    "    if np.sum(selected_features) == 0:\n",
    "        print(\"\\nWarning: No features with VIP > 1, selecting all features\")\n",
    "        selected_features = np.ones_like(selected_features, dtype=bool)\n",
    "        \n",
    "    selected_feature_names = np.array(feature_names)[selected_features]\n",
    "    print(f\"\\nSelected {len(selected_feature_names)} features with VIP > 0.5\")\n",
    "    \n",
    "    return selected_features, selected_feature_names\n",
    "\n",
    "\n",
    "def balance_samples(features, labels):\n",
    "\n",
    "\n",
    "    neg_indices = labels[labels == 0].index\n",
    "    pos_indices = labels[labels == 1].index\n",
    "    \n",
    "\n",
    "    print(\"\\nOriginal sample distribution:\")\n",
    "    print(labels.value_counts())\n",
    "    \n",
    "\n",
    "    min_samples = min(len(neg_indices), len(pos_indices))\n",
    "    \n",
    " \n",
    "    if len(neg_indices) > len(pos_indices):\n",
    "        neg_indices = np.random.choice(neg_indices, min_samples, replace=False)\n",
    "    else:\n",
    "        pos_indices = np.random.choice(pos_indices, min_samples, replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([neg_indices, pos_indices])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "    \n",
    "\n",
    "    features_balanced = features.loc[balanced_indices]\n",
    "    labels_balanced = labels.loc[balanced_indices]\n",
    "    \n",
    " \n",
    "    print(\"\\nBalanced sample distribution:\")\n",
    "    print(labels_balanced.value_counts())\n",
    "    \n",
    "    return features_balanced, labels_balanced\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 200, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 12)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_float('max_features', 0.7, 1.0)\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "    \n",
    " \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Repeated Stratified k-fold cross-validation\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "    fold_results = []\n",
    "    gender_labels = features['gender']\n",
    "    \n",
    "\n",
    "    features_balanced, labels_balanced = balance_samples(features, labels)\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(rskf.split(features_balanced, labels_balanced, gender_labels)):\n",
    "        X_train, X_test = features_balanced.iloc[train_index], features_balanced.iloc[test_index]\n",
    "        y_train, y_test = labels_balanced.iloc[train_index], labels_balanced.iloc[test_index]\n",
    "\n",
    "\n",
    "        print(f\"\\nFold {fold_idx + 1}:\")\n",
    "        print(\"Training set - Positive samples:\", sum(y_train== 1), \", Negative samples:\", sum(y_train == 0))\n",
    "        print(\"Testing set - Positive samples:\", sum(y_test == 1), \", Negative samples:\", sum(y_test == 0))\n",
    "\n",
    "\n",
    "        selected_features, selected_feature_names =pls_feature_selection(X_train, y_train, features_balanced.columns)\n",
    "        X_train_pls = X_train.loc[:, selected_features]\n",
    "        X_test_pls = X_test.loc[:, selected_features]\n",
    "\n",
    "\n",
    "        results = evaluate_model(model, X_train_pls, y_train, X_test_pls, y_test)\n",
    "        train_acc, test_acc, train_sens, test_sens, train_spec, test_spec, train_auroc, test_auroc, train_ppv, test_ppv, train_npv, test_npv, train_f1, test_f1, y_test, y_test_pred, model = results\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold_idx': fold_idx + 1,\n",
    "            'train_index': train_index,\n",
    "            'test_index': test_index,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'train_sens': train_sens,\n",
    "            'test_sens': test_sens,\n",
    "            'train_spec': train_spec,\n",
    "            'test_spec': test_spec,\n",
    "            'train_auroc': train_auroc,\n",
    "            'test_auroc': test_auroc,\n",
    "            'train_ppv': train_ppv,\n",
    "            'test_ppv': test_ppv,\n",
    "            'train_npv': train_npv,\n",
    "            'test_npv': test_npv,\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'selected_feature_names': selected_feature_names,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'model': model\n",
    "        })\n",
    "    \n",
    " \n",
    "    trial.set_user_attr(\"fold_results\", fold_results)\n",
    "    return np.mean([result['test_auroc'] for result in fold_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFS feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from boruta import BorutaPy\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_sens = recall_score(y_train, y_train_pred)\n",
    "    test_sens = recall_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_spec = recall_score(y_train, y_train_pred, pos_label=0)\n",
    "    test_spec = recall_score(y_test, y_test_pred, pos_label=0)\n",
    "    \n",
    "    train_auroc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])\n",
    "    test_auroc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    train_ppv = precision_score(y_train, y_train_pred)\n",
    "    test_ppv = precision_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_npv = precision_score(y_train, y_train_pred, pos_label=0)\n",
    "    test_npv = precision_score(y_test, y_test_pred, pos_label=0)\n",
    "\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    return train_acc, test_acc, train_sens, test_sens, train_spec, test_spec, train_auroc, test_auroc, train_ppv, test_ppv, train_npv, test_npv, train_f1, test_f1, y_test, y_test_pred, model\n",
    "\n",
    "\n",
    "def gfs_feature_selection(X, y, feature_names, estimator=None, scoring='roc_auc', \n",
    "                        max_features=10, min_score_improvement=0.001):\n",
    "    if estimator is None:\n",
    "        estimator = DecisionTreeClassifier(\n",
    "            max_depth=4,\n",
    "            min_samples_split=20,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    " \n",
    "    feature_names = list(feature_names)\n",
    "    selected_features = []\n",
    "    remaining_features = feature_names.copy()\n",
    "    best_score = -np.inf\n",
    "\n",
    "    max_features = min(max_features, len(feature_names))\n",
    "\n",
    "    while len(selected_features) < max_features and remaining_features:\n",
    "        scores = []\n",
    "        for feature in remaining_features:\n",
    "            candidate_features = selected_features + [feature]\n",
    "            X_candidate = X[candidate_features]\n",
    "            \n",
    "            estimator.fit(X_candidate, y)\n",
    "            \n",
    "            if scoring == 'roc_auc':\n",
    "                probs = estimator.predict_proba(X_candidate)[:, 1]\n",
    "                score = roc_auc_score(y, probs)\n",
    "            else:\n",
    "                score = estimator.score(X_candidate, y)\n",
    "            \n",
    "            scores.append((feature, score))\n",
    "        \n",
    "        if not scores:\n",
    "            break\n",
    "            \n",
    "        best_feature, best_current_score = max(scores, key=lambda x: x[1])\n",
    "        \n",
    "        if (best_current_score - best_score) < min_score_improvement:\n",
    "            print(f\"Stopping: Performance improvement < {min_score_improvement}\")\n",
    "            break\n",
    "        \n",
    "        best_score = best_current_score\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        \n",
    "        print(f\"Selected feature: {best_feature} (Score: {best_current_score:.4f})\")\n",
    "    \n",
    "    if not selected_features:\n",
    "        print(\"Warning: No features selected, using first 3 features\")\n",
    "        selected_features = feature_names[:3]\n",
    "    \n",
    "    print(\"\\nFinal selected features:\")\n",
    "    for feature in selected_features:\n",
    "        print(f\"- {feature}\")\n",
    "    print(f\"Number of selected features: {len(selected_features)}\")\n",
    "    print(f\"Best {scoring} score: {best_score:.4f}\")\n",
    "    \n",
    "    return selected_features, selected_features\n",
    "\n",
    "def balance_samples(features, labels):\n",
    "\n",
    "\n",
    "    neg_indices = labels[labels == 0].index\n",
    "    pos_indices = labels[labels == 1].index\n",
    "    \n",
    "\n",
    "    print(\"\\nOriginal sample distribution:\")\n",
    "    print(labels.value_counts())\n",
    "    \n",
    "\n",
    "    min_samples = min(len(neg_indices), len(pos_indices))\n",
    "    \n",
    "\n",
    "    if len(neg_indices) > len(pos_indices):\n",
    "        neg_indices = np.random.choice(neg_indices, min_samples, replace=False)\n",
    "    else:\n",
    "        pos_indices = np.random.choice(pos_indices, min_samples, replace=False)\n",
    "    \n",
    "\n",
    "    balanced_indices = np.concatenate([neg_indices, pos_indices])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "    \n",
    "\n",
    "    features_balanced = features.loc[balanced_indices]\n",
    "    labels_balanced = labels.loc[balanced_indices]\n",
    "    \n",
    "\n",
    "    print(\"\\nBalanced sample distribution:\")\n",
    "    print(labels_balanced.value_counts())\n",
    "    \n",
    "    return features_balanced, labels_balanced\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 200, 500)\n",
    "    max_depth = trial.suggest_int('max_depth', 4, 12)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_float('max_features', 0.7, 1.0)\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "    \n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Repeated Stratified k-fold cross-validation\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "    fold_results = []\n",
    "    gender_labels = features['gender']\n",
    "    \n",
    "\n",
    "    features_balanced, labels_balanced = balance_samples(features, labels)\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(rskf.split(features_balanced, labels_balanced, gender_labels)):\n",
    "        X_train, X_test = features_balanced.iloc[train_index], features_balanced.iloc[test_index]\n",
    "        y_train, y_test = labels_balanced.iloc[train_index], labels_balanced.iloc[test_index]\n",
    "\n",
    "\n",
    "        print(f\"\\nFold {fold_idx + 1}:\")\n",
    "        print(\"Training set - Positive samples:\", sum(y_train== 1), \", Negative samples:\", sum(y_train == 0))\n",
    "        print(\"Testing set - Positive samples:\", sum(y_test == 1), \", Negative samples:\", sum(y_test == 0))\n",
    "\n",
    "\n",
    "        selected_features, selected_feature_names =gfs_feature_selection(X_train, y_train, features_balanced.columns)\n",
    "        X_train_pls = X_train.loc[:, selected_features]\n",
    "        X_test_pls = X_test.loc[:, selected_features]\n",
    "\n",
    "\n",
    "        results = evaluate_model(model, X_train_pls, y_train, X_test_pls, y_test)\n",
    "        train_acc, test_acc, train_sens, test_sens, train_spec, test_spec, train_auroc, test_auroc, train_ppv, test_ppv, train_npv, test_npv, train_f1, test_f1, y_test, y_test_pred, model = results\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold_idx': fold_idx + 1,\n",
    "            'train_index': train_index,\n",
    "            'test_index': test_index,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'train_sens': train_sens,\n",
    "            'test_sens': test_sens,\n",
    "            'train_spec': train_spec,\n",
    "            'test_spec': test_spec,\n",
    "            'train_auroc': train_auroc,\n",
    "            'test_auroc': test_auroc,\n",
    "            'train_ppv': train_ppv,\n",
    "            'test_ppv': test_ppv,\n",
    "            'train_npv': train_npv,\n",
    "            'test_npv': test_npv,\n",
    "            'train_f1': train_f1,\n",
    "            'test_f1': test_f1,\n",
    "            'selected_feature_names': selected_feature_names,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'model': model\n",
    "        })\n",
    "    \n",
    "\n",
    "    trial.set_user_attr(\"fold_results\", fold_results)\n",
    "    return np.mean([result['test_auroc'] for result in fold_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "# Retrieve the best trial and its results\n",
    "best_trial = study.best_trial\n",
    "best_fold_results = best_trial.user_attrs[\"fold_results\"]\n",
    "\n",
    "# Find the best fold based on test AUROC\n",
    "best_fold_idx = np.argmax([result['test_auroc'] for result in best_fold_results])\n",
    "best_fold_result = best_fold_results[best_fold_idx]\n",
    "\n",
    "# Print evaluation metrics of the best fold\n",
    "print(\"\\nBest fold evaluation metrics:\")\n",
    "print(f\"  Fold {best_fold_result['fold_idx']}\")\n",
    "print(f\"  Train Accuracy: {best_fold_result['train_acc']:.4f}\")\n",
    "print(f\"  Test Accuracy: {best_fold_result['test_acc']:.4f}\")\n",
    "print(f\"  Train Sensitivity: {best_fold_result['train_sens']:.4f}\")\n",
    "print(f\"  Test Sensitivity: {best_fold_result['test_sens']:.4f}\")\n",
    "print(f\"  Train Specificity: {best_fold_result['train_spec']:.4f}\")\n",
    "print(f\"  Test Specificity: {best_fold_result['test_spec']:.4f}\")\n",
    "print(f\"  Train AUROC: {best_fold_result['train_auroc']:.4f}\")\n",
    "print(f\"  Test AUROC: {best_fold_result['test_auroc']:.4f}\")\n",
    "print(f\"  Train PPV: {best_fold_result['train_ppv']:.4f}\")\n",
    "print(f\"  Test PPV: {best_fold_result['test_ppv']:.4f}\")\n",
    "print(f\"  Train NPV: {best_fold_result['train_npv']:.4f}\")\n",
    "print(f\"  Test NPV: {best_fold_result['test_npv']:.4f}\")\n",
    "print(f\"  Train F1 Score: {best_fold_result['train_f1']:.4f}\")\n",
    "print(f\"  Test F1 Score: {best_fold_result['test_f1']:.4f}\")\n",
    "\n",
    "# Get the selected features from the best fold\n",
    "selected_features_final = best_fold_result['selected_feature_names']\n",
    "print(\"Selected features for best fold:\", selected_features_final)\n",
    "print(f\"Number of selected features: {len(selected_features_final)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_final=['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Function to compute mean and confidence interval\n",
    "def compute_mean_and_ci(results, metric_idx):\n",
    "    metrics = np.array([result[metric_idx] for result in results])\n",
    "    mean = np.mean(metrics)\n",
    "    ci = 1.96 * np.std(metrics) / np.sqrt(len(metrics))  # 95% Confidence Interval\n",
    "    return mean, ci\n",
    "\n",
    "# Define list of evaluation metrics\n",
    "metrics = ['Train Accuracy', 'Test Accuracy', 'Train Sensitivity', 'Test Sensitivity',\n",
    "           'Train Specificity', 'Test Specificity', 'Train AUROC', 'Test AUROC',\n",
    "           'Train PPV', 'Test PPV', 'Train NPV', 'Test NPV', 'Train F1 Score', 'Test F1 Score']\n",
    "\n",
    "# Retrain the model with best parameters and selected features using 5-fold cross-validation\n",
    "best_params = best_trial.params\n",
    "model = RandomForestClassifier(**best_params)\n",
    "print(\"best_params:\", best_params)\n",
    "\n",
    "# Get selected features\n",
    "# selected_features_final = best_fold_result['selected_feature_names']\n",
    "print(\"Selected features:\", selected_features_final)\n",
    "print(f\"Number of selected features: {len(selected_features_final)}\")\n",
    "\n",
    "# Select features using pandas column names instead of positional indexing\n",
    "X_final = features[selected_features_final]\n",
    "gender_labels = features['gender']\n",
    "\n",
    "# Repeated Stratified K-Fold\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "final_fold_results = []\n",
    "y_test_all = []\n",
    "y_test_pred_all = []\n",
    "final_y_pred_prob = []\n",
    "\n",
    "def balance_samples(X, y, random_state=42):\n",
    "    \"\"\"Balance positive and negative samples\"\"\"\n",
    "    # Convert y to DataFrame if it's a Series\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = pd.DataFrame(y)\n",
    "    else:\n",
    "        y = pd.DataFrame(y, columns=['target'])\n",
    "\n",
    "    # Combine X and y\n",
    "    X_y = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Get label column name\n",
    "    y_col = y.columns[0]\n",
    "\n",
    "    # Separate positive and negative samples\n",
    "    negative_samples = X_y[X_y[y_col] == 0]\n",
    "    positive_samples = X_y[X_y[y_col] == 1]\n",
    "\n",
    "    # Get the number of samples in the minority class\n",
    "    min_samples = min(len(negative_samples), len(positive_samples))\n",
    "\n",
    "    # Undersample the majority class\n",
    "    if len(negative_samples) > min_samples:\n",
    "        negative_samples = resample(negative_samples,\n",
    "                                    n_samples=min_samples,\n",
    "                                    random_state=random_state)\n",
    "    if len(positive_samples) > min_samples:\n",
    "        positive_samples = resample(positive_samples,\n",
    "                                    n_samples=min_samples,\n",
    "                                    random_state=random_state)\n",
    "\n",
    "    # Combine balanced samples\n",
    "    balanced_samples = pd.concat([negative_samples, positive_samples])\n",
    "\n",
    "    # Shuffle the data\n",
    "    balanced_samples = balanced_samples.sample(frac=1, random_state=random_state)\n",
    "\n",
    "    # Split back into features and labels\n",
    "    X_balanced = balanced_samples[X.columns]\n",
    "    y_balanced = balanced_samples[y_col]\n",
    "\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "# Perform repeated stratified k-fold cross-validation ensuring gender and label stratification\n",
    "for fold_idx, (train_index, test_index) in enumerate(rskf.split(X_final, labels, gender_labels)):\n",
    "    X_train_fold_orig, X_test_fold = X_final.iloc[train_index], X_final.iloc[test_index]\n",
    "    y_train_fold_orig, y_test_fold = labels.iloc[train_index], labels.iloc[test_index]\n",
    "\n",
    "    # Balance training set\n",
    "    X_train_fold, y_train_fold = balance_samples(X_train_fold_orig, y_train_fold_orig)\n",
    "    # Balance test set\n",
    "    X_test_fold, y_test_fold = balance_samples(X_test_fold, y_test_fold)\n",
    "\n",
    "    # Output sample counts for current fold\n",
    "    print(f\"\\nFold {fold_idx + 1}:\")\n",
    "    print(\"Training set - Positive samples:\", sum(y_train_fold == 1), \", Negative samples:\", sum(y_train_fold == 0))\n",
    "    print(\"Testing set - Positive samples:\", sum(y_test_fold == 1), \", Negative samples:\", sum(y_test_fold == 0))\n",
    "\n",
    "    # Print features used in this fold\n",
    "    print(\"\\nFeatures used in this fold:\")\n",
    "    for i, feature in enumerate(X_train_fold.columns, 1):\n",
    "        print(f\"{i}. {feature}\")\n",
    "    print(\"-\" * 50)  # Divider for better readability\n",
    "\n",
    "    # Train the model and get prediction probabilities\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    y_test_prob = model.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "    # Collect true labels and predicted probabilities\n",
    "    y_test_all.extend(y_test_fold)\n",
    "    final_y_pred_prob.extend(y_test_prob)\n",
    "\n",
    "    # Evaluate the model\n",
    "    final_results = evaluate_model(model, X_train_fold, y_train_fold, X_test_fold, y_test_fold)\n",
    "    final_fold_results.append(final_results)\n",
    "\n",
    "# Generate final predictions based on probability threshold\n",
    "final_y_pred = np.array([1 if prob >= 0.5 else 0 for prob in final_y_pred_prob])\n",
    "\n",
    "# Output length check\n",
    "print(f\"\\nTotal true labels: {len(y_test_all)}\")\n",
    "print(f\"Total predicted probabilities: {len(final_y_pred_prob)}\")\n",
    "\n",
    "# Print evaluation metrics for each fold\n",
    "for idx, result in enumerate(final_fold_results, 1):\n",
    "    train_acc, test_acc, train_sens, test_sens, train_spec, test_spec, train_auroc, test_auroc, train_ppv, test_ppv, train_npv, test_npv, train_f1, test_f1, y_test, y_test_pred, model = result\n",
    "    print(f\"\\nFold {idx} evaluation metrics after refitting:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Train Sensitivity: {train_sens:.4f}\")\n",
    "    print(f\"  Test Sensitivity: {test_sens:.4f}\")\n",
    "    print(f\"  Train Specificity: {train_spec:.4f}\")\n",
    "    print(f\"  Test Specificity: {test_spec:.4f}\")\n",
    "    print(f\"  Train AUROC: {train_auroc:.4f}\")\n",
    "    print(f\"  Test AUROC: {test_auroc:.4f}\")\n",
    "    print(f\"  Train PPV: {train_ppv:.4f}\")\n",
    "    print(f\"  Test PPV: {test_ppv:.4f}\")\n",
    "    print(f\"  Train NPV: {train_npv:.4f}\")\n",
    "    print(f\"  Test NPV: {test_npv:.4f}\")\n",
    "    print(f\"  Train F1: {train_f1:.4f}\")\n",
    "    print(f\"  Test F1: {test_f1:.4f}\")\n",
    "\n",
    "# Compute and print average performance across folds\n",
    "print(\"\\nOverall Performance:\")\n",
    "for idx, metric in enumerate(metrics):\n",
    "    mean, ci = compute_mean_and_ci(final_fold_results, idx)\n",
    "    print(f'{metric}: Mean = {mean:.3f}, 95% CI = [{mean - ci:.3f}, {mean + ci:.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF-SHAP summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Global plot settings\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.labelsize'] = 26\n",
    "plt.rcParams['xtick.labelsize'] = 26\n",
    "plt.rcParams['ytick.labelsize'] = 26\n",
    "plt.rcParams['legend.fontsize'] = 26\n",
    "FEATURE_FONT_SIZE = 14\n",
    "\n",
    "def create_shap_plots(X_resampled, y_resampled, selected_features_final, best_params, rskf):\n",
    "    \"\"\"Generate SHAP plots for each fold\"\"\"\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(rskf.split(X_resampled, y_resampled)):\n",
    "        print(f'Fold {fold_idx+1}: SHAP values')\n",
    "        \n",
    "        # Get fold data\n",
    "        X_train_fold_orig = X_resampled.iloc[train_index]\n",
    "        y_train_fold_orig = y_resampled[train_index]\n",
    "        X_test_fold_orig = X_resampled.iloc[test_index]\n",
    "        y_test_fold_orig = y_resampled[test_index]\n",
    "        \n",
    "        # Balance datasets\n",
    "        X_train_fold, y_train_fold = balance_samples(X_train_fold_orig, y_train_fold_orig)\n",
    "        X_test_fold, y_test_fold = balance_samples(X_test_fold_orig, y_test_fold_orig)\n",
    "        \n",
    "        try:\n",
    "            # Train model with best parameters\n",
    "            model = RandomForestClassifier(**best_params)\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Calculate SHAP values\n",
    "            explainer = shap.TreeExplainer(model, feature_perturbation='interventional')\n",
    "            shap_values = explainer.shap_values(X_test_fold)\n",
    "            \n",
    "            # Ensure correct shape for plotting\n",
    "            if isinstance(shap_values, np.ndarray) and len(shap_values.shape) == 3:\n",
    "                shap_values = [shap_values[:,:,0], shap_values[:,:,1]]\n",
    "            \n",
    "            # Debug SHAP values\n",
    "            print(\"Number of features:\", len(X_test_fold.columns))\n",
    "            print(\"SHAP values type:\", type(shap_values))\n",
    "            if isinstance(shap_values, list):\n",
    "                print(\"SHAP values shapes:\", [sv.shape for sv in shap_values])\n",
    "            else:\n",
    "                print(\"SHAP values shape:\", shap_values.shape)\n",
    "\n",
    "            # Print shapes for debugging\n",
    "            print(\"SHAP_value shape:\", np.array(shap_values).shape if isinstance(shap_values, list) else shap_values.shape)\n",
    "            print(\"X_test shape:\", X_test_fold.shape)\n",
    "            print(\"Features:\", list(X_test_fold.columns))\n",
    "            \n",
    "            # Summary plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            with plt.style.context({'font.family': 'Times New Roman'}):\n",
    "                shap_values_plot = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "                shap.summary_plot(\n",
    "                    shap_values_plot,\n",
    "                    X_test_fold,\n",
    "                    show=False,\n",
    "                    plot_size=(10, 6),\n",
    "                    max_display=len(X_test_fold.columns)  \n",
    "                )\n",
    "                \n",
    "                ax = plt.gca()\n",
    "                ax.set_title(f'SHAP Summary Plot (Top 10) - Fold {fold_idx+1}', \n",
    "                           fontfamily='Times New Roman', \n",
    "                           fontsize=12, \n",
    "                           pad=20)\n",
    "                ax.set_xlabel(ax.get_xlabel(), fontfamily='Times New Roman', fontsize=14)\n",
    "                ax.set_ylabel(ax.get_ylabel(), fontfamily='Times New Roman', fontsize=12)\n",
    "                ax.tick_params(axis='y', labelsize=FEATURE_FONT_SIZE)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Bar plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            with plt.style.context({'font.family': 'Times New Roman'}):\n",
    "                shap.summary_plot(\n",
    "                    shap_values_plot,\n",
    "                    X_test_fold,\n",
    "                    plot_type=\"bar\",\n",
    "                    show=False,\n",
    "                    plot_size=(10, 6),\n",
    "                    max_display=len(X_test_fold.columns)  \n",
    "                )\n",
    "             \n",
    "                ax = plt.gca()\n",
    "                bars = ax.patches  \n",
    "                for bar in bars:\n",
    "                    bar.set_facecolor('blue')  \n",
    "                ax.set_title(f'SHAP Bar Plot (Top 10) - Fold {fold_idx+1}', \n",
    "                fontfamily='Times New Roman', fontsize=12, pad=20)\n",
    "                ax.set_xlabel(ax.get_xlabel(), fontfamily='Times New Roman', fontsize=14)\n",
    "                ax.set_ylabel(ax.get_ylabel(), fontfamily='Times New Roman', fontsize=12)\n",
    "                ax.tick_params(axis='both', which='major', labelsize=16, labelrotation=0)\n",
    "                ax.tick_params(axis='y', labelsize=FEATURE_FONT_SIZE)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Force plot for example instances\n",
    "            positive_indices = np.where(y_resampled == 1)[0]\n",
    "            negative_indices = np.where(y_resampled == 0)[0]\n",
    "            \n",
    "            if len(positive_indices) > 0 and len(negative_indices) > 0:\n",
    "                first_positive = X_test_fold.iloc[0]\n",
    "                first_negative = X_test_fold.iloc[-1]\n",
    "                \n",
    "                shap_values_pos = explainer.shap_values(first_positive)\n",
    "                shap_values_neg = explainer.shap_values(first_negative)\n",
    "                \n",
    "                plt.figure(figsize=(12, 4))\n",
    "                shap.force_plot(\n",
    "                    explainer.expected_value,\n",
    "                    shap_values_pos[1] if isinstance(shap_values_pos, list) else shap_values_pos,\n",
    "                    first_positive,\n",
    "                    show=False,\n",
    "                    matplotlib=True\n",
    "                )\n",
    "                plt.title(\"Force Plot - Positive Example\", fontfamily='Times New Roman', fontsize=12)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold_idx+1}:\", str(e))\n",
    "            print(\"Shapes:\", X_train_fold.shape, y_train_fold.shape, X_test_fold.shape, y_test_fold.shape)\n",
    "            break\n",
    "\n",
    "# Usage\n",
    "X_resampled = X_final[selected_features_final]\n",
    "create_shap_plots(X_resampled, labels, selected_features_final, best_params, rskf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF-CIC curve chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "def calculate_metrics_at_threshold(y_true, y_prob, threshold):\n",
    "    \"\"\"Calculate performance metrics at a given threshold\"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    total_samples = len(y_true)\n",
    "    scaling_factor = 1000 / total_samples\n",
    "    \n",
    "    n_high_risk = (fp + tp) * scaling_factor\n",
    "    n_high_risk_with_event = tp * scaling_factor\n",
    "    \n",
    "    return n_high_risk, n_high_risk_with_event\n",
    "\n",
    "def plot_cost_benefit_curve(y_true, y_prob):\n",
    "    \"\"\"Plot cost-benefit curve\"\"\"\n",
    "    # Create a smaller figure with more space at the bottom\n",
    "    fig, ax = plt.subplots(figsize=(4, 3.2), dpi=300)\n",
    "    \n",
    "    # Set background color to white\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    # Generate threshold sequence\n",
    "    thresholds = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    n_high_risk_list = []\n",
    "    n_high_risk_with_event_list = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        n_high_risk, n_high_risk_with_event = calculate_metrics_at_threshold(\n",
    "            y_true, y_prob, threshold\n",
    "        )\n",
    "        n_high_risk_list.append(n_high_risk)\n",
    "        n_high_risk_with_event_list.append(n_high_risk_with_event)\n",
    "    \n",
    "    # Plot main curves with labels\n",
    "    ax.plot(thresholds, n_high_risk_list, color='red', linestyle='-', \n",
    "            linewidth=1.0, label='Number high risk')\n",
    "    ax.plot(thresholds, n_high_risk_with_event_list, color='blue', \n",
    "            linestyle='--', linewidth=1.0, label='Number high risk with event')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(fontsize=8, loc='upper right', frameon=False)\n",
    "    \n",
    "    # Set axis labels with smaller font\n",
    "    ax.set_xlabel('High Risk Threshold', fontsize=9)\n",
    "    ax.set_ylabel('Number high risk (out of 1000)', fontsize=9)\n",
    "    \n",
    "    # Set axis limits\n",
    "    ax.set_ylim(0, 1000)\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Set smaller tick font size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    \n",
    "    # Add cost-benefit ratio labels\n",
    "    cost_benefit_ratios = ['1:100', '1:4', '2:3', '3:2', '4:1', '100:1']\n",
    "    ratio_positions = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    \n",
    "    # Create secondary x-axis at the bottom\n",
    "    ax2 = ax.secondary_xaxis(-0.25)\n",
    "    ax2.set_xlim(ax.get_xlim())\n",
    "    ax2.set_xticks(ratio_positions)\n",
    "    ax2.set_xticklabels(cost_benefit_ratios, fontsize=8)\n",
    "    \n",
    "    # Set label for secondary axis\n",
    "    ax2.set_xlabel('Cost:Benefit Ratio', fontsize=9, labelpad=8)\n",
    "    \n",
    "    # Set tick parameters for secondary axis\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=8)\n",
    "    \n",
    "    # Set border line width\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(0.5)\n",
    "    \n",
    "    # Hide unnecessary spines for ax2\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['left'].set_visible(False)\n",
    "    \n",
    "    # Adjust margins to make room for bottom axis\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Use real prediction results from the model\n",
    "y_true = np.array(y_test_all)\n",
    "y_prob = np.array(final_y_pred_prob)\n",
    "\n",
    "# Plot the curve\n",
    "plt = plot_cost_benefit_curve(y_true, y_prob)\n",
    "\n",
    "# Save the smaller figure\n",
    "plt.savefig('cost_benefit_curve.png', dpi=300, bbox_inches='tight', \n",
    "            format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### six machine learning models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, confusion_matrix, \n",
    "                           precision_score, recall_score, f1_score)\n",
    "import numpy as np\n",
    "\n",
    "# XGBoost model selected features and hyperparameters\n",
    "# best_params: {'n_estimators': 201, 'max_depth': 9, 'learning_rate': 0.1335376701461377, 'min_child_weight': 6, \n",
    "# 'gamma': 0.21064265390797143, 'subsample': 0.895162492770872, 'colsample_bytree': 0.7272554168734067, \n",
    "# 'scale_pos_weight': 1.1277357372234167, 'reg_alpha': 0.5503700132271422, 'reg_lambda': 0.6075194736990024}\n",
    "# Selected features: ['age' 'aniongap' 'bun' 'creatinine' 'wbc' 'rbc' 'weight'\n",
    "#  'hr' 'rr' 'temperature' 'glucose' 'gcs']\n",
    "# Number of selected features: 12\n",
    "def objective_xgboost(trial):\n",
    "    # Use the best parameters provided in comments\n",
    "    params = {\n",
    "        'n_estimators': 201,\n",
    "        'max_depth': 9,\n",
    "        'learning_rate': 0.1335376701461377,\n",
    "        'min_child_weight': 6,\n",
    "        'gamma': 0.21064265390797143,\n",
    "        'subsample': 0.895162492770872,\n",
    "        'colsample_bytree': 0.7272554168734067,\n",
    "        'scale_pos_weight': 1.1277357372234167,\n",
    "        'reg_alpha': 0.5503700132271422,\n",
    "        'reg_lambda': 0.6075194736990024,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    Selected_features = ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    return params, Selected_features\n",
    "\n",
    "\n",
    "# LightGBM model selected features and hyperparameters\n",
    "# best_params: {'learning_rate': 0.047669762018487424, 'n_estimators': 79, 'num_leaves': 45, 'max_depth': 7, \n",
    "# 'min_child_samples': 7, 'feature_fraction': 0.8057018384484658, 'bagging_fraction': 0.706785747058316}\n",
    "# Selected features: ['age' 'aniongap' 'bun' 'creatinine' 'wbc' 'rbc' 'hr'\n",
    "#  'rr' 'temperature' 'glucose' 'gcs']\n",
    "# Number of selected features: 11\n",
    "def objective_lightgbm(trial):\n",
    "    # Use the best parameters provided in comments\n",
    "    params = {\n",
    "        'learning_rate': 0.047669762018487424,\n",
    "        'n_estimators': 79,\n",
    "        'num_leaves': 45,\n",
    "        'max_depth': 7,\n",
    "        'min_child_samples': 7,\n",
    "        'feature_fraction': 0.8057018384484658,\n",
    "        'bagging_fraction': 0.706785747058316,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    Selected_features = ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    return params, Selected_features\n",
    "\n",
    "\n",
    "# Random Forest model selected features and hyperparameters\n",
    "# best_params: {'n_estimators': 141, 'max_depth': 9, \n",
    "#               'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'log2'}\n",
    "# Selected features: ['age' 'aniongap' 'bun' 'creatinine' 'wbc' 'rbc' 'weight'\n",
    "#  'hr' 'rr' 'temperature' 'glucose' 'gcs']\n",
    "# Number of selected features: 12\n",
    "def objective_random_forest(trial):\n",
    "    # Use the best parameters provided in comments\n",
    "    params = {\n",
    "        'n_estimators': 141,\n",
    "        'max_depth': 9,\n",
    "        'min_samples_split': 10,\n",
    "        'min_samples_leaf': 6,\n",
    "        'max_features': 'log2',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    Selected_features = ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    return params, Selected_features\n",
    "\n",
    "\n",
    "# SVM model selected features and hyperparameters\n",
    "# best_params: {'C': 3.8606754376312318}\n",
    "# Selected features: ['age' 'aniongap' 'bun' 'creatinine' 'wbc' 'rbc' 'weight'\n",
    "#  'hr' 'rr' 'temperature' 'glucose' 'gcs']\n",
    "# Number of selected features: 12\n",
    "def objective_svm(trial):\n",
    "    params = {\n",
    "        'C': 3.8606754376312318,\n",
    "        'gamma': 'scale',\n",
    "        'kernel': 'rbf',\n",
    "        'probability': True,\n",
    "        'random_state': 42,\n",
    "        'cache_size': 1000\n",
    "    }\n",
    "    Selected_features = ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    return params, Selected_features\n",
    "\n",
    "\n",
    "# AdaBoost model selected features and hyperparameters\n",
    "# best_params: {'n_estimators': 177, 'learning_rate': 0.420580705420142}\n",
    "# Selected features: ['aniongap' 'bun' 'creatinine' 'wbc' 'rbc' 'hr' 'rr'\n",
    "#  'temperature' 'glucose' 'gcs']\n",
    "# Number of selected features: 10\n",
    "def objective_adaboost(trial):\n",
    "    params = {\n",
    "        'n_estimators': 177,\n",
    "        'learning_rate': 0.420580705420142,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    Selected_features = ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    return params, Selected_features\n",
    "\n",
    "\n",
    "# Logistic Regression model selected features and hyperparameters\n",
    "# best_params: {'C': 2.195022414600841, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "# Selected features: ['age' 'aniongap' 'bun' 'creatinine' 'wbc' 'rbc' 'hr'\n",
    "#  'rr' 'temperature' 'glucose' 'gcs']\n",
    "# Number of selected features: 11\n",
    "def objective_logistic(trial):\n",
    "    params = {\n",
    "        'C': 1.9351172214826404,\n",
    "        'penalty': 'l2',\n",
    "        'solver': 'liblinear',\n",
    "        'max_iter': 500,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    Selected_features = ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    return params, Selected_features\n",
    "\n",
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = precision_score(y_true, y_pred)\n",
    "    npv = tn / (tn + fn)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auroc = roc_auc_score(y_true, y_prob)\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'AUROC': auroc,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "\n",
    "# Get optimized parameters and features for each model\n",
    "xgb_params, xgb_features = objective_xgboost(None)\n",
    "lgb_params, lgb_features = objective_lightgbm(None)\n",
    "rf_params, rf_features = objective_random_forest(None)\n",
    "svm_params, svm_features = objective_svm(None)\n",
    "ada_params, ada_features = objective_adaboost(None)\n",
    "lr_params, lr_features = objective_logistic(None)\n",
    "\n",
    "# Define models with optimized parameters\n",
    "models = {\n",
    "    'XGBoost': {'model': XGBClassifier(**xgb_params), 'features': xgb_features},\n",
    "    'LightGBM': {'model': LGBMClassifier(**lgb_params), 'features': lgb_features},\n",
    "    'AdaBoost': {'model': AdaBoostClassifier(**ada_params), 'features': ada_features},\n",
    "    'RandomForest': {'model': RandomForestClassifier(**rf_params), 'features': rf_features},\n",
    "    'SVM': {'model': SVC(**svm_params), 'features': svm_features},\n",
    "    'LogisticRegression': {'model': LogisticRegression(**lr_params), 'features': lr_features},\n",
    "}\n",
    "\n",
    "model_name = ['XGBoost','LightGBM','RandomForest','AdaBoost','SVM','AdaBoost','LogisticRegression']\n",
    "\n",
    "# Dictionary to store evaluation metrics for all models\n",
    "all_models_metrics = {model_name: {\n",
    "    'train_metrics': [],\n",
    "    'test_metrics': []\n",
    "} for model_name in models.keys()}\n",
    "\n",
    "\n",
    "# Function to compute mean and confidence interval\n",
    "def compute_mean_and_ci(results, metric_idx):\n",
    "    metrics = np.array([result[metric_idx] for result in results])\n",
    "    mean = np.mean(metrics)\n",
    "    ci = 1.96 * np.std(metrics) / np.sqrt(len(metrics))  # 95% confidence interval\n",
    "    return mean, ci\n",
    "\n",
    "\n",
    "# List of evaluation metrics\n",
    "metrics = ['Train Accuracy', 'Test Accuracy', 'Train Sensitivity', 'Test Sensitivity', \n",
    "          'Train Specificity', 'Test Specificity', 'Train AUROC', 'Test AUROC', \n",
    "          'Train PPV', 'Test PPV', 'Train NPV', 'Test NPV', 'Train F1 Score', 'Test F1 Score']\n",
    "\n",
    "\n",
    "# Dictionary to store evaluation metrics for all models\n",
    "all_models_metrics = {model_name: {\n",
    "    'train_metrics': [],\n",
    "    'test_metrics': []\n",
    "} for model_name in models.keys()}\n",
    "\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model_info in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model = model_info['model']\n",
    "    features = model_info['features']\n",
    "    \n",
    "    # Check if features exist\n",
    "    available_features = []\n",
    "    missing_features = []\n",
    "    for feature in features:\n",
    "        if feature in X_final.columns:\n",
    "            available_features.append(feature)\n",
    "        else:\n",
    "            missing_features.append(feature)\n",
    "    if missing_features:\n",
    "        print(f\"Warning: Following features are missing for {model_name}: {missing_features}\")\n",
    "        print(f\"Will proceed with available features: {available_features}\")\n",
    "        features = available_features\n",
    "    \n",
    "    # Reset result storage for each model\n",
    "    final_fold_results = []\n",
    "    y_test_all = []\n",
    "    y_test_pred_all = []\n",
    "    final_y_pred_prob = []\n",
    "\n",
    "    # Create subset with selected features\n",
    "    X_selected = X_final[features]\n",
    "    print(\"Available features in dataset:\")\n",
    "    print(X_final.columns.tolist())\n",
    "\n",
    "    # Repeated Stratified K-Fold\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "    for fold_idx, (train_index, test_index) in enumerate(rskf.split(X_selected, labels, gender_labels)):\n",
    "        print(f\"\\nFold {fold_idx + 1}:\")\n",
    "        X_train_fold_orig = X_selected.iloc[train_index]\n",
    "        X_test_fold = X_selected.iloc[test_index]\n",
    "        y_train_fold_orig = labels.iloc[train_index]\n",
    "        y_test_fold = labels.iloc[test_index]\n",
    "\n",
    "        # Balance samples\n",
    "        X_train_fold, y_train_fold = balance_samples(X_train_fold_orig, y_train_fold_orig)\n",
    "        X_test_fold, y_test_fold = balance_samples(X_test_fold, y_test_fold)\n",
    "\n",
    "        # Output number of positive/negative samples in current fold\n",
    "        print(\"Training set - Positive samples:\", sum(y_train_fold == 1), \", Negative samples:\", sum(y_train_fold == 0))\n",
    "        print(\"Testing set - Positive samples:\", sum(y_test_fold == 1), \", Negative samples:\", sum(y_test_fold == 0))\n",
    "\n",
    "        # Print features used in this fold\n",
    "        print(\"\\nFeatures used in this fold:\")\n",
    "        for i, feature in enumerate(X_train_fold.columns, 1):\n",
    "            print(f\"{i}. {feature}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        final_results = evaluate_model(model, X_train_fold, y_train_fold, X_test_fold, y_test_fold)\n",
    "        final_fold_results.append(final_results)\n",
    "\n",
    "        # Collect prediction results\n",
    "        y_test_all.extend(y_test_fold)\n",
    "        if isinstance(model, SVC):\n",
    "            y_test_prob = model.decision_function(X_test_fold)\n",
    "            y_test_prob = 1 / (1 + np.exp(-y_test_prob))\n",
    "        else:\n",
    "            y_test_prob = model.predict_proba(X_test_fold)[:, 1]\n",
    "        final_y_pred_prob.extend(y_test_prob)\n",
    "\n",
    "    # Final output of label lengths\n",
    "    print(f\"\\nTotal true labels: {len(y_test_all)}\")\n",
    "    print(f\"Total predicted probabilities: {len(final_y_pred_prob)}\")\n",
    "\n",
    "    # Print evaluation metrics for each fold\n",
    "    for idx, result in enumerate(final_fold_results, 1):\n",
    "        train_acc, test_acc, train_sens, test_sens, train_spec, test_spec, train_auroc, test_auroc, train_ppv, test_ppv, train_npv, test_npv, train_f1, test_f1, y_test, y_test_pred, model = result\n",
    "        print(f\"\\nFold {idx} evaluation metrics after refitting:\")\n",
    "        print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"  Train Sensitivity: {train_sens:.4f}\")\n",
    "        print(f\"  Test Sensitivity: {test_sens:.4f}\")\n",
    "        print(f\"  Train Specificity: {train_spec:.4f}\")\n",
    "        print(f\"  Test Specificity: {test_spec:.4f}\")\n",
    "        print(f\"  Train AUROC: {train_auroc:.4f}\")\n",
    "        print(f\"  Test AUROC: {test_auroc:.4f}\")\n",
    "        print(f\"  Train PPV: {train_ppv:.4f}\")\n",
    "        print(f\"  Test PPV: {test_ppv:.4f}\")\n",
    "        print(f\"  Train NPV: {train_npv:.4f}\")\n",
    "        print(f\"  Test NPV: {test_npv:.4f}\")\n",
    "        print(f\"  Train F1: {train_f1:.4f}\")\n",
    "        print(f\"  Test F1: {test_f1:.4f}\")\n",
    "\n",
    "    # Store results for this model\n",
    "    all_models_metrics[model_name] = {\n",
    "        'final_fold_results': final_fold_results,\n",
    "        'y_test_all': y_test_all,\n",
    "        'final_y_pred_prob': final_y_pred_prob\n",
    "    }\n",
    "\n",
    "# Print comparison results for all models\n",
    "print(\"\\nModel Comparison:\")\n",
    "for model_name in models.keys():\n",
    "    results = all_models_metrics[model_name]['final_fold_results']\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        mean, ci = compute_mean_and_ci(results, idx)\n",
    "        print(f'{metric}: Mean = {mean:.3f}, 95% CI = [{mean-ci:.3f}, {mean+ci:.3f}]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve of six machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "def plot_roc_curves(models, all_models_metrics):\n",
    "\n",
    "   plt.rcParams.update({\n",
    "       'font.family': 'Times New Roman',\n",
    "       'font.serif': ['Times New Roman'],\n",
    "       'font.size': 12,\n",
    "       'axes.labelsize': 16, \n",
    "       'axes.titlesize': 16,\n",
    "       'xtick.labelsize': 16,\n",
    "       'ytick.labelsize': 16,\n",
    "       'legend.fontsize': 14\n",
    "   })\n",
    "\n",
    "   plt.figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "   colors = {\n",
    "       'XGBoost': '#4B7BE5',\n",
    "       'LightGBM': '#2E8B57',\n",
    "       'RandomForest': '#DC3445',\n",
    "        'AdaBoost': '#A0522D',\n",
    "        'SVM': '#FFA500',\n",
    "       'LogisticRegression': '#9B51E0', \n",
    "   }\n",
    "\n",
    "\n",
    "   model_aucs = {}\n",
    "   for model_name in models.keys():\n",
    "       results = all_models_metrics[model_name]['final_fold_results']\n",
    "       test_aurocs = [result[7] for result in results]\n",
    "       mean_auc = np.mean(test_aurocs)\n",
    "       std_auc = np.std(test_aurocs)\n",
    "       model_aucs[model_name] = (mean_auc, std_auc)\n",
    "\n",
    "   ax = plt.gca()\n",
    "   ax.set_facecolor('white')\n",
    "   plt.gcf().set_facecolor('white')\n",
    "\n",
    "   for model_name in models.keys():\n",
    "       y_test = all_models_metrics[model_name]['y_test_all']\n",
    "       y_pred_prob = all_models_metrics[model_name]['final_y_pred_prob']\n",
    "       \n",
    "       fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "       mean_auc, std_auc = model_aucs[model_name]\n",
    "       \n",
    "       plt.plot(fpr, tpr,\n",
    "                color=colors[model_name],\n",
    "                label=f'{model_name} (AUC = {mean_auc:.3f} ± {std_auc:.3f})',\n",
    "                linewidth=1.5)\n",
    "\n",
    "   plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1.5)\n",
    "   plt.xlim([0, 1])\n",
    "   plt.ylim([0, 1])\n",
    "\n",
    "   plt.xlabel('False Positive Rate (1 - Specificity)', fontfamily='Times New Roman')\n",
    "   plt.ylabel('True Positive Rate (Sensitivity)', fontfamily='Times New Roman')\n",
    "   plt.title('Average ROC Curves for Different Models', fontfamily='Times New Roman', pad=10)\n",
    "\n",
    "   plt.legend(loc=\"lower right\",\n",
    "             prop={'family': 'Times New Roman'},\n",
    "             frameon=False,\n",
    "             edgecolor='black')\n",
    "\n",
    "   plt.xticks(fontfamily='Times New Roman')\n",
    "   plt.yticks(fontfamily='Times New Roman')\n",
    "\n",
    "   for spine in ax.spines.values():\n",
    "       spine.set_linewidth(1)\n",
    "\n",
    "   #plt.grid(True, linestyle=':', alpha=0.3)\n",
    "   plt.tight_layout()\n",
    "   return plt\n",
    "\n",
    "plt = plot_roc_curves(models, all_models_metrics)  \n",
    "plt.savefig('roc_curves_mimic_predict_eicu.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix diagram of six machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrices(all_models_metrics):\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Times New Roman',\n",
    "        'font.serif': ['Times New Roman'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 16,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 16,\n",
    "        'ytick.labelsize': 16,\n",
    "        'legend.fontsize': 14\n",
    "    })\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10), dpi=300)\n",
    "    fig.suptitle('Confusion Matrices for Different Models', \n",
    "                 fontsize=20, \n",
    "                 y=1.05, \n",
    "                 fontfamily='Times New Roman')\n",
    "\n",
    "    cmap = plt.cm.Blues\n",
    "\n",
    "    for idx, (model_name, metrics) in enumerate(all_models_metrics.items()):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "        ax.set_facecolor('white')\n",
    "        \n",
    "        # 从metrics中获取真实标签和预测概率\n",
    "        y_true = np.array(metrics['y_test_all'])\n",
    "        y_pred_prob = np.array(metrics['final_y_pred_prob'])\n",
    "        fold_size = len(y_true) // 5\n",
    "        \n",
    "        cm_sum = np.zeros((2, 2))\n",
    "        \n",
    "        for i in range(5):\n",
    "            start_idx = i * fold_size\n",
    "            end_idx = (i + 1) * fold_size if i < 4 else len(y_true)\n",
    "            fold_y_true = y_true[start_idx:end_idx]\n",
    "            fold_y_pred = (y_pred_prob[start_idx:end_idx] > 0.5).astype(int)\n",
    "            cm = confusion_matrix(fold_y_true, fold_y_pred)\n",
    "            cm_sum += cm\n",
    "        \n",
    "        cm = (cm_sum / 5).astype(int)\n",
    "        total = cm.sum()\n",
    "        cm_percent = cm / total * 100\n",
    "        cm_norm = cm.astype('float') / cm.max()\n",
    "        \n",
    "        im = ax.imshow(cm_norm, interpolation='nearest', \n",
    "                      cmap=cmap,\n",
    "                      aspect='auto')\n",
    "        \n",
    "        # 修改这里：对于浅色背景使用黑色文字，深色背景使用白色文字\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                # 调整阈值，使得浅色区域都使用黑色文字\n",
    "                color = \"white\" if cm_norm[i, j] > 0.7 else \"black\"\n",
    "                # 显示数值\n",
    "                ax.text(j, i, str(cm[i, j]),\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=color,\n",
    "                       fontsize=14,\n",
    "                       fontfamily='Times New Roman',\n",
    "                       fontweight='bold')\n",
    "                # 显示百分比\n",
    "                ax.text(j, i + 0.25,\n",
    "                       f'({cm_percent[i,j]:.1f}%)',\n",
    "                       ha='center', \n",
    "                       va='center',\n",
    "                       color=color,\n",
    "                       fontsize=12,\n",
    "                       fontfamily='Times New Roman')\n",
    "        \n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels(['Predicted\\nNegative', 'Predicted\\nPositive'],\n",
    "                          fontfamily='Times New Roman', \n",
    "                          fontsize=14)\n",
    "        ax.set_yticklabels(['Actual\\nNegative', 'Actual\\nPositive'],\n",
    "                          fontfamily='Times New Roman', \n",
    "                          fontsize=14)\n",
    "        \n",
    "        ax.set_title(f'{model_name}', \n",
    "                    pad=10, \n",
    "                    fontsize=16, \n",
    "                    fontfamily='Times New Roman')\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(1.0)\n",
    "            \n",
    "        ax.set_xticks(np.arange(-0.5, 2, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-0.5, 2, 1), minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=1)\n",
    "        ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# example:\n",
    "plt = plot_confusion_matrices(all_models_metrics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCA decision curve graph of six machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dca_curves(all_models_metrics, title=\"Model Comparison - DCA Curves\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Set global font settings\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Times New Roman',\n",
    "        'font.serif': ['Times New Roman'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 16, \n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 16,\n",
    "        'ytick.labelsize': 16,\n",
    "        'legend.fontsize': 14\n",
    "    })\n",
    "\n",
    "    # Define color mapping for models\n",
    "    colors = {\n",
    "        'XGBoost': '#2E86C1',\n",
    "        'LightGBM': '#27AE60',\n",
    "        'RandomForest': '#8E44AD',\n",
    "        'SVM': '#E67E22',\n",
    "        'AdaBoost': '#C0392B',\n",
    "        'LogisticRegression': '#34495E'\n",
    "    }\n",
    "    \n",
    "    # Generate threshold probabilities\n",
    "    thresholds = np.arange(0, 1.01, 0.01)\n",
    "    \n",
    "    # Calculate and plot DCA curves for each model\n",
    "    for model_name, metrics in all_models_metrics.items():\n",
    "        y_true = np.array(metrics['y_test_all'])\n",
    "        y_pred_prob = np.array(metrics['final_y_pred_prob'])\n",
    "        \n",
    "        # Ensure the lengths are consistent\n",
    "        min_len = min(len(y_true), len(y_pred_prob))\n",
    "        y_true = y_true[:min_len]\n",
    "        y_pred_prob = y_pred_prob[:min_len]\n",
    "        \n",
    "        # Compute net benefit for each threshold\n",
    "        net_benefits = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "            TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "            FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "            n = len(y_true)\n",
    "            \n",
    "            if TP + FP == 0:\n",
    "                net_benefit = 0\n",
    "            else:\n",
    "                net_benefit = (TP / n) - (FP / n) * (threshold / (1 - threshold))\n",
    "            net_benefits.append(net_benefit)\n",
    "            \n",
    "        plt.plot(thresholds, net_benefits, '-', color=colors[model_name], \n",
    "                 linewidth=1.5, label=model_name)\n",
    "    \n",
    "    # Plot baseline: treat-all and treat-none strategies\n",
    "    y_true = np.array(metrics['y_test_all'])  # Reuse from last model\n",
    "    all_positives = np.ones_like(y_true)\n",
    "    net_benefits_all = []\n",
    "    for threshold in thresholds:\n",
    "        TP = np.sum(y_true == 1)\n",
    "        FP = np.sum(y_true == 0)\n",
    "        n = len(y_true)\n",
    "        net_benefit = (TP / n) - (FP / n) * (threshold / (1 - threshold))\n",
    "        net_benefits_all.append(net_benefit)\n",
    "    \n",
    "    plt.plot(thresholds, net_benefits_all, '--', color='black', \n",
    "             linewidth=1.5, label='Treat All')\n",
    "    plt.plot(thresholds, np.zeros_like(thresholds), '--', color='gray', \n",
    "             linewidth=1.5, label='Treat None')\n",
    "    \n",
    "    # Set axis labels\n",
    "    plt.xlabel('Threshold Probability', fontsize=16)\n",
    "    plt.ylabel('Net Benefit', fontsize=16)\n",
    "    plt.title(title, fontsize=16, pad=15)\n",
    "\n",
    "    # Set tick label sizes\n",
    "    plt.tick_params(axis='x', labelsize=16)\n",
    "    plt.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    # Set legend\n",
    "    plt.legend(loc='upper right', frameon=False, facecolor='white', \n",
    "               edgecolor='black', fontsize=14)\n",
    "    \n",
    "    # Set axis limits\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(min(min(net_benefits), -0.05), \n",
    "             max(max(net_benefits_all), max(net_benefits)) + 0.05)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Set border line width\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "    return plt.gcf()\n",
    "\n",
    "# Example usage:\n",
    "plot_dca_curves(all_models_metrics, \"Model Comparison - DCA Curves\")\n",
    "plt.savefig('dca_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load eICU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, confusion_matrix, \n",
    "                             precision_score, recall_score, f1_score)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Read the dataset\n",
    "file_path = \"E:\\\\eicu_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Print number of samples before preprocessing\n",
    "print(\"Number of samples before data preprocessing:\", len(data))\n",
    "\n",
    "# Data preprocessing\n",
    "filtered_data1 = data[data['age'] >= 18]\n",
    "print(\"Number of samples after data preprocessing (age >= 18):\", len(filtered_data1))\n",
    "\n",
    "filtered_data2 = filtered_data1[filtered_data1['myocardial_infarct'] != 1]\n",
    "\n",
    "print(\"Number of samples after data preprocessing (non-myocardial infarction):\", len(filtered_data2))\n",
    "\n",
    "filtered_data3 = filtered_data2[filtered_data2['congestive_heart_failure'] != 1]\n",
    "print(\"Number of samples after data preprocessing (non-congestive heart failure):\", len(filtered_data3))\n",
    "\n",
    "filtered_data4 = filtered_data3[filtered_data3['icu_los_hours'] >= 3]\n",
    "print(\"Number of samples after data preprocessing (ICU length of stay >= 3 hours):\", len(filtered_data4))\n",
    "\n",
    "# Final processed dataset\n",
    "external_filtered_data = filtered_data4\n",
    "print(\"Final number of samples after data preprocessing:\", len(external_filtered_data))\n",
    "\n",
    "# Print label distribution in the final dataset\n",
    "label_distribution = external_filtered_data['label'].value_counts()\n",
    "print(\"Label distribution in the final dataset:\")\n",
    "print(label_distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features obtained from MIMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selected_features = ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "# Extract features and labels\n",
    "X = external_filtered_data[Selected_features]  \n",
    "y = external_filtered_data['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set-evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "\n",
    "# XGBoost trained hyperparameters\n",
    "best_params = {'n_estimators': 255, 'max_depth': 10, 'learning_rate': 0.09273015415736849,\n",
    "               'min_child_weight': 2, 'gamma': 0.6038476986548106, 'subsample': 0.7316641746804098,\n",
    "               'colsample_bytree': 0.8006388061568654, 'scale_pos_weight': 1.050569703507208,\n",
    "               'reg_alpha': 0.1437966726288049, 'reg_lambda': 0.3236297624663411}\n",
    "\n",
    "# Features selected by PLS-XGBoost\n",
    "XGB_Selected_features = ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = precision_score(y_true, y_pred)\n",
    "    npv = tn / (tn + fn)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auroc = roc_auc_score(y_true, y_prob)\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'AUROC': auroc,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_data(X_train, X_test):\n",
    "    # Create imputer for missing values in numerical features\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    # Create scaler for numerical features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Get numerical and categorical columns\n",
    "    numerical_columns = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_columns = X_train.select_dtypes(include=['category']).columns\n",
    "\n",
    "    # Make copies to avoid modifying original data\n",
    "    X_train_processed = X_train.copy()\n",
    "    X_test_processed = X_test.copy()\n",
    "\n",
    "    # Process numerical features\n",
    "    if len(numerical_columns) > 0:\n",
    "        # Fit and transform on training data\n",
    "        X_train_num = num_imputer.fit_transform(X_train_processed[numerical_columns])\n",
    "        X_train_num = scaler.fit_transform(X_train_num)\n",
    "\n",
    "        # Transform on test data\n",
    "        X_test_num = num_imputer.transform(X_test_processed[numerical_columns])\n",
    "        X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "        # Convert back to DataFrame\n",
    "        X_train_processed[numerical_columns] = X_train_num\n",
    "        X_test_processed[numerical_columns] = X_test_num\n",
    "\n",
    "    # Process categorical features\n",
    "    if len(categorical_columns) > 0:\n",
    "        # Fill missing values with mode for categorical columns\n",
    "        for col in categorical_columns:\n",
    "            mode_value = X_train_processed[col].mode()[0]\n",
    "            X_train_processed[col] = X_train_processed[col].fillna(mode_value)\n",
    "            X_test_processed[col] = X_test_processed[col].fillna(mode_value)\n",
    "\n",
    "    return X_train_processed, X_test_processed\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance on training and test sets\"\"\"\n",
    "    # Evaluate on training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    if isinstance(model, SVC):\n",
    "        y_train_prob = model.decision_function(X_train)\n",
    "        y_train_prob = 1 / (1 + np.exp(-y_train_prob))\n",
    "    else:\n",
    "        y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_prob)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    if isinstance(model, SVC):\n",
    "        y_test_prob = model.decision_function(X_test)\n",
    "        y_test_prob = 1 / (1 + np.exp(-y_test_prob))\n",
    "    else:\n",
    "        y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_prob)\n",
    "\n",
    "    return (train_metrics['Accuracy'], test_metrics['Accuracy'],\n",
    "            train_metrics['Sensitivity'], test_metrics['Sensitivity'],\n",
    "            train_metrics['Specificity'], test_metrics['Specificity'],\n",
    "            train_metrics['AUROC'], test_metrics['AUROC'],\n",
    "            train_metrics['PPV'], test_metrics['PPV'],\n",
    "            train_metrics['NPV'], test_metrics['NPV'],\n",
    "            train_metrics['F1 Score'], test_metrics['F1 Score'],\n",
    "            y_test, y_test_pred,\n",
    "            model)\n",
    "\n",
    "\n",
    "def balance_samples(X, y, random_state=42):\n",
    "    \"\"\"Balance positive and negative samples\"\"\"\n",
    "    # Convert y to DataFrame if needed\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = pd.DataFrame(y)\n",
    "    else:\n",
    "        y = pd.DataFrame(y, columns=['target'])\n",
    "\n",
    "    # Combine X and y\n",
    "    X_y = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Get label column name\n",
    "    y_col = y.columns[0]\n",
    "\n",
    "    # Separate positive and negative samples\n",
    "    negative_samples = X_y[X_y[y_col] == 0]\n",
    "    positive_samples = X_y[X_y[y_col] == 1]\n",
    "\n",
    "    # Get the number of minority class\n",
    "    min_samples = min(len(negative_samples), len(positive_samples))\n",
    "\n",
    "    # Downsample majority class\n",
    "    if len(negative_samples) > min_samples:\n",
    "        negative_samples = resample(negative_samples,\n",
    "                                    n_samples=min_samples,\n",
    "                                    random_state=random_state)\n",
    "    if len(positive_samples) > min_samples:\n",
    "        positive_samples = resample(positive_samples,\n",
    "                                    n_samples=min_samples,\n",
    "                                    random_state=random_state)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    balanced_samples = pd.concat([negative_samples, positive_samples])\n",
    "    balanced_samples = balanced_samples.sample(frac=1, random_state=random_state)\n",
    "\n",
    "    # Split back into features and labels\n",
    "    X_balanced = balanced_samples[X.columns]\n",
    "    y_balanced = balanced_samples[y_col]\n",
    "\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "models = {\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(\n",
    "            n_estimators=255,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.09273015415736849,\n",
    "            min_child_weight=2,\n",
    "            gamma=0.6038476986548106,\n",
    "            subsample=0.7316641746804098,\n",
    "            colsample_bytree=0.8006388061568654,\n",
    "            scale_pos_weight=1.050569703507208,\n",
    "            reg_alpha=0.1437966726288049,\n",
    "            reg_lambda=0.3236297624663411,\n",
    "            random_state=42,\n",
    "            enable_categorical=True\n",
    "        ),\n",
    "        'features': ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': LGBMClassifier(\n",
    "            learning_rate=0.042628740412729516,\n",
    "            n_estimators=163,\n",
    "            num_leaves=46,\n",
    "            max_depth=5,\n",
    "            min_child_samples=15,\n",
    "            feature_fraction=0.8611486887182361,\n",
    "            bagging_fraction=0.8397657244664654,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'features': ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': AdaBoostClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'features': ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=186,\n",
    "            max_depth=10,\n",
    "            min_samples_split=9,\n",
    "            min_samples_leaf=3,\n",
    "            max_features='log2',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'features': ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(\n",
    "            C=0.1,\n",
    "            gamma='scale',\n",
    "            kernel='rbf',\n",
    "            probability=True,\n",
    "            random_state=42,\n",
    "            cache_size=1000\n",
    "        ),\n",
    "        'features': ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(\n",
    "            C=0.1,\n",
    "            penalty='l2',\n",
    "            solver='saga',\n",
    "            max_iter=500,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'features': ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def compute_mean_and_ci(results, idx):\n",
    "    \"\"\"Compute mean and 95% confidence interval\"\"\"\n",
    "    values = [result[idx] for result in results]\n",
    "    mean = np.mean(values)\n",
    "    if len(values) < 2:\n",
    "        return mean, 0\n",
    "    try:\n",
    "        sem = stats.sem(values)\n",
    "        confidence_interval = stats.t.interval(confidence=0.95,\n",
    "                                               df=len(values)-1,\n",
    "                                               loc=mean,\n",
    "                                               scale=sem)\n",
    "        ci = confidence_interval[1] - mean\n",
    "    except:\n",
    "        ci = 0\n",
    "    return mean, ci\n",
    "\n",
    "\n",
    "# Dictionary to store evaluation metrics for all models\n",
    "all_models_metrics = {model_name: {\n",
    "    'final_fold_results': [],\n",
    "    'y_test_all': [],\n",
    "    'final_y_pred_prob': []\n",
    "} for model_name in models.keys()}\n",
    "\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model_info in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "\n",
    "    if model_name == 'XGBoost':\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=255,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.09273015415736849,\n",
    "            min_child_weight=2,\n",
    "            gamma=0.6038476986548106,\n",
    "            subsample=0.7316641746804098,\n",
    "            colsample_bytree=0.8006388061568654,\n",
    "            scale_pos_weight=1.050569703507208,\n",
    "            reg_alpha=0.1437966726288049,\n",
    "            reg_lambda=0.3236297624663411,\n",
    "            random_state=42,\n",
    "            enable_categorical=True\n",
    "        )\n",
    "    elif model_name == 'LightGBM':\n",
    "        model = LGBMClassifier(\n",
    "            learning_rate=0.042628740412729516,\n",
    "            n_estimators=163,\n",
    "            num_leaves=46,\n",
    "            max_depth=5,\n",
    "            min_child_samples=15,\n",
    "            feature_fraction=0.8611486887182361,\n",
    "            bagging_fraction=0.8397657244664654,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif model_name == 'RandomForest':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=186,\n",
    "            max_depth=10,\n",
    "            min_samples_split=9,\n",
    "            min_samples_leaf=3,\n",
    "            max_features='log2',\n",
    "            random_state=42\n",
    "        )\n",
    "    elif model_name == 'SVM':\n",
    "        model = SVC(\n",
    "            C=0.1,\n",
    "            gamma='scale',\n",
    "            kernel='rbf',\n",
    "            probability=True,\n",
    "            random_state=42,\n",
    "            cache_size=1000\n",
    "        )\n",
    "    elif model_name == 'AdaBoost':\n",
    "        model = AdaBoostClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif model_name\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set-ROC curve of six machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def plot_roc_curves(all_models_metrics):\n",
    "    \"\"\"Plot ROC curves for all models with mean AUC and standard deviation\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "    # Define colors for each model\n",
    "    colors = {\n",
    "        'XGBoost': '#4B7BE5',\n",
    "        'LightGBM': '#2E8B57',\n",
    "        'AdaBoost': '#A0522D',\n",
    "        'RandomForest': '#DC3445',\n",
    "        'SVM': '#FFA500',\n",
    "        'LogisticRegression': '#9B51E0', \n",
    "    }\n",
    "\n",
    "    # Set global font settings\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Times New Roman',\n",
    "        'font.serif': ['Times New Roman'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 16, \n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 16,\n",
    "        'ytick.labelsize': 16,\n",
    "        'legend.fontsize': 12\n",
    "    })\n",
    "\n",
    "    # Plot ROC curve for each model\n",
    "    for model_name, metrics in all_models_metrics.items():\n",
    "        # Get true labels and predicted probabilities\n",
    "        y_test = np.array(metrics['y_test_all'])\n",
    "        y_pred_prob = np.array(metrics['final_y_pred_prob'])\n",
    "        \n",
    "        # Compute ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "        \n",
    "        # Calculate mean AUROC and its standard deviation across folds\n",
    "        auroc_values = [result[7] for result in metrics['final_fold_results']]\n",
    "        mean_auroc = np.mean(auroc_values)\n",
    "        std_auroc = np.std(auroc_values)\n",
    "\n",
    "        # Plot the ROC curve\n",
    "        plt.plot(fpr, tpr, \n",
    "                 color=colors.get(model_name),  \n",
    "                 linestyle='-',\n",
    "                 lw=2,\n",
    "                 label=f'{model_name} (AUC = {mean_auroc:.3f} ± {std_auroc:.3f})')\n",
    "    \n",
    "    # Plot diagonal line (no-skill classifier)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1.5, alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.title('ROC Curves Comparison', fontsize=14, pad=20)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='lower right', fontsize=12, bbox_to_anchor=(1.0, 0.0),\n",
    "               borderaxespad=1, framealpha=0.8, frameon=False)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "# Example usage\n",
    "fig = plot_roc_curves(all_models_metrics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set-Confusion matrix diagram of six machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrices(all_models_metrics):\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Times New Roman',\n",
    "        'font.serif': ['Times New Roman'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 16,\n",
    "        'axes.titlesize': 16,\n",
    "        'xtick.labelsize': 16,\n",
    "        'ytick.labelsize': 16,\n",
    "        'legend.fontsize': 14\n",
    "    })\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10), dpi=300)\n",
    "    fig.suptitle('Average Confusion Matrices for Different Models\\n(5-fold CV × 10 repeats)', \n",
    "                 fontsize=20, \n",
    "                 y=1.05, \n",
    "                 fontfamily='Times New Roman')\n",
    "\n",
    "    cmap = plt.cm.Blues\n",
    "\n",
    "    for idx, (model_name, metrics) in enumerate(all_models_metrics.items()):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "        ax.set_facecolor('white')\n",
    "        \n",
    "        # 从metrics中获取数据\n",
    "        y_true = np.array(metrics['y_test_all'])\n",
    "        y_pred_prob = np.array(metrics['final_y_pred_prob'])\n",
    "        \n",
    "        # 分成50份（5折×10重复）\n",
    "        n_splits = 50\n",
    "        fold_size = len(y_true) // n_splits\n",
    "        cms = []\n",
    "        \n",
    "        # 计算每一折的混淆矩阵\n",
    "        for i in range(n_splits):\n",
    "            start_idx = i * fold_size\n",
    "            end_idx = (i + 1) * fold_size if i < n_splits-1 else len(y_true)\n",
    "            fold_y_true = y_true[start_idx:end_idx]\n",
    "            fold_y_pred = (y_pred_prob[start_idx:end_idx] > 0.5).astype(int)\n",
    "            cm = confusion_matrix(fold_y_true, fold_y_pred)\n",
    "            cms.append(cm)\n",
    "        \n",
    "        # 计算平均混淆矩阵\n",
    "        cm = np.mean(cms, axis=0).round().astype(int)\n",
    "        total = cm.sum()\n",
    "        cm_percent = cm / total * 100\n",
    "        cm_norm = cm.astype('float') / cm.max()\n",
    "        \n",
    "        im = ax.imshow(cm_norm, interpolation='nearest', \n",
    "                      cmap=cmap,\n",
    "                      aspect='auto')\n",
    "        \n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                color = \"white\" if cm_norm[i, j] > 0.7 else \"black\"\n",
    "                # 显示平均值\n",
    "                ax.text(j, i, str(cm[i, j]),\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=color,\n",
    "                       fontsize=14,\n",
    "                       fontfamily='Times New Roman',\n",
    "                       fontweight='bold')\n",
    "                # 显示百分比\n",
    "                ax.text(j, i + 0.25,\n",
    "                       f'({cm_percent[i,j]:.1f}%)',\n",
    "                       ha='center', \n",
    "                       va='center',\n",
    "                       color=color,\n",
    "                       fontsize=12,\n",
    "                       fontfamily='Times New Roman')\n",
    "        \n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels(['Predicted\\nNegative', 'Predicted\\nPositive'],\n",
    "                          fontfamily='Times New Roman', \n",
    "                          fontsize=14)\n",
    "        ax.set_yticklabels(['Actual\\nNegative', 'Actual\\nPositive'],\n",
    "                          fontfamily='Times New Roman', \n",
    "                          fontsize=14)\n",
    "        \n",
    "        ax.set_title(f'{model_name}', \n",
    "                    pad=10, \n",
    "                    fontsize=16, \n",
    "                    fontfamily='Times New Roman')\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(1.0)\n",
    "            \n",
    "        ax.set_xticks(np.arange(-0.5, 2, 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-0.5, 2, 1), minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=1)\n",
    "        ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrices_50_folds_average.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# 使用示例:\n",
    "plt = plot_confusion_matrices(all_models_metrics)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set-DCA decision curve graph of six machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_net_benefit(y_true, y_pred_prob, threshold):\n",
    "    \"\"\"计算特定阈值下的Net Benefit\"\"\"\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    n = len(y_true)\n",
    "    \n",
    "    if TP + FP == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (TP/n - (FP/n) * (threshold/(1-threshold)))\n",
    "\n",
    "def plot_dca_curves(all_models_metrics):\n",
    "    # 创建图形和轴对象\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # 设置全局样式\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['axes.linewidth'] = 1.5\n",
    "    plt.rcParams.update({\n",
    "       'font.family': 'Times New Roman',\n",
    "       'font.serif': ['Times New Roman'],\n",
    "       'font.size': 12,\n",
    "       'axes.labelsize': 16, \n",
    "       'axes.titlesize': 16,\n",
    "       'xtick.labelsize': 16,\n",
    "       'ytick.labelsize': 16,\n",
    "       'legend.fontsize': 14\n",
    "   })\n",
    "    # 设置颜色映射\n",
    "    colors = {\n",
    "        'XGBoost': '#2E86C1',\n",
    "        'LightGBM': '#27AE60',\n",
    "        'RandomForest': '#8E44AD',\n",
    "        'SVM': '#E67E22',\n",
    "        'AdaBoost': '#C0392B',\n",
    "        'LogisticRegression': '#34495E'\n",
    "    }\n",
    "    \n",
    "    # 设置白色背景\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # 定义阈值范围\n",
    "    thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "    \n",
    "    # 为每个模型计算和绘制DCA曲线\n",
    "    for idx, (model_name, metrics) in enumerate(all_models_metrics.items()):\n",
    "        y_true = np.array(metrics['y_test_all'])\n",
    "        y_pred_prob = np.array(metrics['final_y_pred_prob'])\n",
    "        \n",
    "        # 计算net benefits\n",
    "        net_benefits = [calculate_net_benefit(y_true, y_pred_prob, threshold) \n",
    "                       for threshold in thresholds]\n",
    "        \n",
    "        # 绘制DCA曲线\n",
    "        ax.plot(thresholds, net_benefits, \n",
    "                color=list(colors.values())[idx],\n",
    "                linewidth=2, \n",
    "                label=model_name)\n",
    "    \n",
    "    # 添加treat-all line\n",
    "    treat_all_net_benefits = [calculate_net_benefit(y_true, \n",
    "                                                  np.ones_like(y_true), \n",
    "                                                  threshold)\n",
    "                             for threshold in thresholds]\n",
    "    ax.plot(thresholds, treat_all_net_benefits, color='black',linestyle='--', linewidth=1.5, label='Treat All')\n",
    "    \n",
    "    # 添加treat-none line\n",
    "    ax.plot([0, 1], [0, 0], color='gray',linestyle='--', linewidth=1.5, label='Treat None')\n",
    "    \n",
    "    # 设置坐标轴范围和标签\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([-0.05, 0.6])  # 调整y轴范围\n",
    "    ax.set_xlabel('Threshold Probability', fontsize=16)\n",
    "    ax.set_ylabel('Net Benefit', fontsize=16)\n",
    "    ax.set_title('Decision Curve Analysis', pad=15, fontsize=14)\n",
    "    \n",
    "    # 设置图例\n",
    "    ax.legend(loc='upper right', \n",
    "              bbox_to_anchor=(1.0, 1.0),\n",
    "              fontsize=12,\n",
    "              frameon=False)\n",
    "    \n",
    "    # 设置刻度\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    \n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图片\n",
    "    plt.savefig('dca_curves_adjusted.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# 使用示例：\n",
    "plt = plot_dca_curves(all_models_metrics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set-SHAP summary plot for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取RF模型最后一个fold的训练结果\n",
    "rf_results = all_models_metrics['RandomForest']['final_fold_results'][-1]\n",
    "final_model = rf_results[-1]  # 获取模型\n",
    "\n",
    "# 准备特征列表\n",
    "features = ['age', 'pt', 'ptt', 'aniongap', 'bun', 'creatinine', 'wbc', 'mchc', 'rbc']\n",
    "\n",
    "# 为了更好地显示，选择一部分样本\n",
    "sample_size = 200\n",
    "X_sample = X_test_processed[features].sample(n=min(sample_size, len(X_test_processed)), random_state=42)\n",
    "\n",
    "# 设置matplotlib参数\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Times New Roman',\n",
    "    'font.size': 20,\n",
    "    'axes.labelsize': 20,\n",
    "    'axes.titlesize': 20,\n",
    "    'xtick.labelsize': 26,\n",
    "    'ytick.labelsize': 26\n",
    "})\n",
    "\n",
    "# 计算SHAP值\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# 由于是二分类问题，我们选择正类的SHAP值\n",
    "# 确保正确的维度\n",
    "if isinstance(shap_values, list):\n",
    "    print(\"Original SHAP values shapes:\", [sv.shape for sv in shap_values])\n",
    "    shap_values_plot = shap_values[1]  # 选择正类的SHAP值\n",
    "    expected_value = explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value\n",
    "else:\n",
    "    print(\"Original SHAP values shape:\", shap_values.shape)\n",
    "    shap_values_plot = shap_values[:,:,1] if len(shap_values.shape) == 3 else shap_values\n",
    "    expected_value = explainer.expected_value\n",
    "\n",
    "print(\"Final SHAP values shape:\", shap_values_plot.shape)\n",
    "print(\"X_sample shape:\", X_sample.shape)\n",
    "\n",
    "# 绘制Summary Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(\n",
    "    shap_values_plot,\n",
    "    X_sample,\n",
    "    max_display=len(features),\n",
    "    show=False,\n",
    "    plot_size=None\n",
    ")\n",
    "# plt.xticks(fontsize=20)\n",
    "# plt.yticks(fontsize=20)\n",
    "# 获取当前轴对象\n",
    "ax = plt.gca()\n",
    "# 设置 x 轴和 y 轴刻度字体大小\n",
    "ax.tick_params(axis='x', labelsize=20)  # x 轴刻度字体大小\n",
    "ax.tick_params(axis='y', labelsize=20)  # y 轴刻度字体大小\n",
    "\n",
    "# 设置 X 和 Y 轴标签\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", fontsize=20, labelpad=20)  # 设置 X 轴标签\n",
    "ax.set_ylabel(\"Features\", fontsize=20, labelpad=20)  # 设置 Y 轴标签\n",
    "# 直接通过 ax 设置标题\n",
    "ax.set_title('Random Forest Model SHAP Summary Plot', fontsize=20, pad=20)\n",
    "# 获取 colorbar（SHAP 颜色条）并调整字体大小\n",
    "cbar = plt.gcf().axes[-1]  # 获取 colorbar 的 axis\n",
    "cbar.tick_params(labelsize=20)  # 设置 colorbar 刻度字体大小\n",
    "cbar.set_ylabel(\"Feature Value\", fontsize=24)  # 设置 colorbar 标签字体大小\n",
    "\n",
    "# 调整 colorbar 上 \"Low\" 和 \"High\" 文字的字体大小\n",
    "for text in cbar.get_yticklabels():  \n",
    "    text.set_fontsize(20)  # 设置 colorbar 刻度字体大小\n",
    "\n",
    "#plt.title('Random Forest Model SHAP Summary Plot', fontsize=24,pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set-SHAP bar plot for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个特征的平均绝对SHAP值\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 计算每个特征的平均绝对SHAP值\n",
    "mean_shap_values = np.abs(shap_values_plot).mean(axis=0)\n",
    "\n",
    "# 将特征和对应的平均SHAP值存入DataFrame中\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_sample.columns,\n",
    "    'Mean SHAP Value': mean_shap_values\n",
    "})\n",
    "\n",
    "# 按照Mean SHAP值排序\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Mean SHAP Value', ascending=True)\n",
    "\n",
    "# 绘制条形图\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Mean SHAP Value'], color='blue')\n",
    "plt.xlabel('Mean Absolute SHAP Value', fontsize=20)\n",
    "plt.ylabel('Features', fontsize=20)\n",
    "#plt.title('Feature Importance based on Mean Absolute SHAP Value', fontsize=24)\n",
    "\n",
    "# 设置轴标签的字体大小\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "# 显示图形\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
